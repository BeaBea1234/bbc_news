{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bbc_bert_farm.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPoJ964lbsWIYiCUQshZnRM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f909fbb6e3434296b75ff3c18ce62158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f1d88c151fd948d291ec5a5e3c91ee09",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_454dce4b4f59404299b46a26aed371f9",
              "IPY_MODEL_6cc377dd61db4e1d990ae24bdf322a36"
            ]
          }
        },
        "f1d88c151fd948d291ec5a5e3c91ee09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "454dce4b4f59404299b46a26aed371f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f05a6de7d93446e494eff8d0af4f4873",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e0827f6534fa45b98d76a8eac523f93a"
          }
        },
        "6cc377dd61db4e1d990ae24bdf322a36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_32d94e87764642048f472bcf072e3d61",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:00&lt;00:00, 1.34MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6a0078355610429381d731c529306322"
          }
        },
        "f05a6de7d93446e494eff8d0af4f4873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e0827f6534fa45b98d76a8eac523f93a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "32d94e87764642048f472bcf072e3d61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6a0078355610429381d731c529306322": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fdbbe3b71aa24b89af699dbbf73f4883": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3e23057359954845b14f96cc70caa297",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bd0959de7eba423e80242f1276d36545",
              "IPY_MODEL_86c6fbb89ee14cdfb832648ecfdfd897"
            ]
          }
        },
        "3e23057359954845b14f96cc70caa297": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bd0959de7eba423e80242f1276d36545": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9e61b0e40b2045c2974eb18d925bcecd",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e0347805542f4008adce831054bf1616"
          }
        },
        "86c6fbb89ee14cdfb832648ecfdfd897": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c8f0abbd6da34fb595464072f2f7435c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 2.04kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6d83ee74dda645eda0da9af278d3ab3f"
          }
        },
        "9e61b0e40b2045c2974eb18d925bcecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e0347805542f4008adce831054bf1616": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c8f0abbd6da34fb595464072f2f7435c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6d83ee74dda645eda0da9af278d3ab3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7eb5ba9026f84ca899581c262d76edc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a7e35606265a471c901e71d678e7d3a3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a10ed0c08166479e85d8a3bcf54b66db",
              "IPY_MODEL_4641f0c406714a69aac7b0a28cab5045"
            ]
          }
        },
        "a7e35606265a471c901e71d678e7d3a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a10ed0c08166479e85d8a3bcf54b66db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_31f71400dfb54817ad08426fd2c64daa",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 435779157,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 435779157,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_50c4d226138c40a8aeb29ff710ec29f2"
          }
        },
        "4641f0c406714a69aac7b0a28cab5045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_67f8df2427b441d2862216e90da93f5c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 436M/436M [00:06&lt;00:00, 67.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_129c98966aa2475ba8f0b346e6b271b7"
          }
        },
        "31f71400dfb54817ad08426fd2c64daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "50c4d226138c40a8aeb29ff710ec29f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "67f8df2427b441d2862216e90da93f5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "129c98966aa2475ba8f0b346e6b271b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guggio/bbc_news/blob/master/bbc_bert_farm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSsOUMsd7zdL",
        "colab_type": "text"
      },
      "source": [
        "# BBC Article Genre Classification with BERT using the FARM Framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YL8miY87u-h",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJCtjihD39AK",
        "colab_type": "code",
        "outputId": "eb5fe1a8-e849-4d33-da0a-1ce949897292",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install farm==0.4.3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting farm==0.4.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/a9/b1f1ff65af01d5cd1d6df698e0c142ab3164afb1189b7cecd8075fee853b/farm-0.4.3.tar.gz (153kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from farm==0.4.3) (46.4.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.6/dist-packages (from farm==0.4.3) (0.34.2)\n",
            "Collecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 23kB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from farm==0.4.3) (4.41.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from farm==0.4.3) (1.13.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from farm==0.4.3) (2.23.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.6/dist-packages (from farm==0.4.3) (1.4.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from farm==0.4.3) (0.0)\n",
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Collecting mlflow==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/ec/8c9448968d4662e8354b9c3a62e635f8929ed507a45af3d9fdb84be51270/mlflow-1.0.0-py3-none-any.whl (47.7MB)\n",
            "\u001b[K     |████████████████████████████████| 47.7MB 56kB/s \n",
            "\u001b[?25hCollecting transformers==2.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/ba/dda44bbf35b071441635708a3dd568a5ca6bf29f77389f7c7c6818ae9498/transformers-2.7.0-py3-none-any.whl (544kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 56.6MB/s \n",
            "\u001b[?25hCollecting dotmap==1.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/eb/ee5f0358a9e0ede90308d8f34e697e122f191c2702dc4f614eca7770b1eb/dotmap-1.3.0-py3-none-any.whl\n",
            "Collecting Werkzeug==0.16.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/e4/a859d2fe516f466642fa5c6054fd9646271f9da26b0cac0d2f37fc858c8f/Werkzeug-0.16.1-py2.py3-none-any.whl (327kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 56.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.6/dist-packages (from farm==0.4.3) (1.1.2)\n",
            "Collecting flask-restplus\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/a6/b17c848771f96ad039ad9e3ea275e842a16c39c4f3eb9f60ee330b20b6c2/flask_restplus-0.13.0-py2.py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 54.6MB/s \n",
            "\u001b[?25hCollecting flask-cors\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from farm==0.4.3) (0.3.1.1)\n",
            "Collecting onnxruntime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/37/572986fb63e0df4e026c5f4c11f6a8977344293587b451d9210a429f5882/onnxruntime-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (3.9MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 51.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->farm==0.4.3) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->farm==0.4.3) (1.16.13)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->farm==0.4.3) (0.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->farm==0.4.3) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->farm==0.4.3) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->farm==0.4.3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->farm==0.4.3) (1.24.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy>=1.3.2->farm==0.4.3) (1.18.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->farm==0.4.3) (0.22.2.post1)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->farm==0.4.3) (2.3.1)\n",
            "Collecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/1e/cabc75a189de0fbb2841d0975243e59bde8b7822bacbb95008ac6fe9ad47/alembic-1.4.2.tar.gz (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 54.1MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.4.3) (1.3.17)\n",
            "Collecting gitpython>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/33/917e6fde1cad13daa7053f39b7c8af3be287314f75f1b1ea8d3fe37a8571/GitPython-3.1.2-py3-none-any.whl (451kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 46.0MB/s \n",
            "\u001b[?25hCollecting querystring-parser\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/fa/f54f5662e0eababf0c49e92fd94bf178888562c0e7b677c8941bbbcd1bd6/querystring_parser-1.2.4.tar.gz\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.4.3) (3.13)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.4.3) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.4.3) (3.10.0)\n",
            "Collecting docker>=3.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/74/379a9d30b1620def158c40b88c43e01c1936a287ebb97afab0699c601c57/docker-4.2.0-py2.py3-none-any.whl (143kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 56.6MB/s \n",
            "\u001b[?25hCollecting databricks-cli>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/d1/fe0ba3d5c2b4b76ec035aa243bbc2fd0d60607a391f192ebe1656e17a4e2/databricks-cli-0.10.0.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlparse in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.4.3) (0.3.1)\n",
            "Collecting simplejson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/a7b98aa9256c8843f92878966dc3d8d914c14aad97e2c5ce4798d5743e07/simplejson-3.17.0.tar.gz (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 13.3MB/s \n",
            "\u001b[?25hCollecting gunicorn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/ca/926f7cd3a2014b16870086b2d0fdc84a9e49473c68a8dff8b57f7c156f43/gunicorn-20.0.4-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.4.3) (0.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.4.3) (1.12.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.4.3) (1.0.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.4.3) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from mlflow==1.0.0->farm==0.4.3) (2.8.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 49.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.7.0->farm==0.4.3) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.7.0->farm==0.4.3) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 48.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.7.0->farm==0.4.3) (0.7)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 56.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask->farm==0.4.3) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask->farm==0.4.3) (2.11.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from flask-restplus->farm==0.4.3) (2018.9)\n",
            "Collecting aniso8601>=0.82\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/e4/787e104b58eadc1a710738d4e418d7e599e4e778e52cb8e5d5ef6ddd5833/aniso8601-8.0.0-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from flask-restplus->farm==0.4.3) (2.6.0)\n",
            "Collecting onnx>=1.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/ee/bc7bc88fc8449266add978627e90c363069211584b937fd867b0ccc59f09/onnx-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 53.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->farm==0.4.3) (0.15.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->farm==0.4.3) (0.15.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->farm==0.4.3) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->farm==0.4.3) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->farm==0.4.3) (1.0.8)\n",
            "Collecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/78/f6ade1e18aebda570eed33b7c534378d9659351cadce2fcbc7b31be5f615/Mako-1.1.2-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.4MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.8MB/s \n",
            "\u001b[?25hCollecting websocket-client>=0.32.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 48.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.6/dist-packages (from databricks-cli>=0.8.0->mlflow==1.0.0->farm==0.4.3) (0.8.7)\n",
            "Collecting configparser>=0.3.5\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask->farm==0.4.3) (1.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx>=1.2.3->onnxruntime->farm==0.4.3) (3.6.6)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: alembic\n",
            "  Building wheel for alembic (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.2-cp36-none-any.whl size=159543 sha256=99778f8aeefd8bcca202593a7eced75ccc3607e2a445e1a5fc91bc56c5c11c40\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/04/83/76023f7a4c14688c0b5c2682a96392cfdd3ee4449eaaa287ef\n",
            "Successfully built alembic\n",
            "Building wheels for collected packages: farm, seqeval, querystring-parser, databricks-cli, simplejson, sacremoses\n",
            "  Building wheel for farm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for farm: filename=farm-0.4.3-cp36-none-any.whl size=170527 sha256=5d3dc896db89175ce5a345c15f701a4d334641ad5e64a7051343248b32edde88\n",
            "  Stored in directory: /root/.cache/pip/wheels/da/78/5c/4ac30f81d1a400d82915e2f63d3935f801f130dfa3d2a156d2\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=fb4bc293214427ec18b6d0bb748d08d8cea67d77917bbea9477c93f0d9aa80a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "  Building wheel for querystring-parser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for querystring-parser: filename=querystring_parser-1.2.4-cp36-none-any.whl size=7079 sha256=e541ae4e942798d57386ad2cbb4968855ff83f02f19be36566fe502f07d1e04f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/41/34/23ebf5d1089a9aed847951e0ee375426eb4ad0a7079d88d41e\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.10.0-cp36-none-any.whl size=84285 sha256=add2f89a9468cd72cbac0d9fa43a7a552df69ac24f7da06bea97ab738f1c2e7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/e5/2d/a19c0bfd38005176063f130d72de17cb3d2d32c0ee384e7493\n",
            "  Building wheel for simplejson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for simplejson: filename=simplejson-3.17.0-cp36-cp36m-linux_x86_64.whl size=114204 sha256=bc0aba7faf2df48551b3f9cf7f514e9eb904cca3dfeca69ae313e0d3f42be620\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/c0/83/dcd0339abb2640544bb8e0938aab2d069cef55e5647ce6e097\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=827e16193daed8cb3ede0e0b9b191c43e2cc9da9b3da96f4095b67d974efed2d\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built farm seqeval querystring-parser databricks-cli simplejson sacremoses\n",
            "\u001b[31mERROR: torchvision 0.6.0+cu101 has requirement torch==1.5.0, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, seqeval, python-editor, Mako, alembic, smmap, gitdb, gitpython, querystring-parser, websocket-client, docker, configparser, databricks-cli, simplejson, gunicorn, mlflow, sacremoses, sentencepiece, tokenizers, transformers, dotmap, Werkzeug, aniso8601, flask-restplus, flask-cors, onnx, onnxruntime, farm\n",
            "  Found existing installation: torch 1.5.0+cu101\n",
            "    Uninstalling torch-1.5.0+cu101:\n",
            "      Successfully uninstalled torch-1.5.0+cu101\n",
            "  Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "Successfully installed Mako-1.1.2 Werkzeug-0.16.1 alembic-1.4.2 aniso8601-8.0.0 configparser-5.0.0 databricks-cli-0.10.0 docker-4.2.0 dotmap-1.3.0 farm-0.4.3 flask-cors-3.0.8 flask-restplus-0.13.0 gitdb-4.0.5 gitpython-3.1.2 gunicorn-20.0.4 mlflow-1.0.0 onnx-1.7.0 onnxruntime-1.3.0 python-editor-1.0.4 querystring-parser-1.2.4 sacremoses-0.0.43 sentencepiece-0.1.91 seqeval-0.0.12 simplejson-3.17.0 smmap-3.0.4 tokenizers-0.5.2 torch-1.4.0 transformers-2.7.0 websocket-client-0.57.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syvMLND-fRwv",
        "colab_type": "code",
        "outputId": "f8976fbe-24cd-4998-b1f6-debed17d9a1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/guggio/bbc_news"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bbc_news'...\n",
            "remote: Enumerating objects: 2190, done.\u001b[K\n",
            "remote: Counting objects: 100% (2190/2190), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2168/2168), done.\u001b[K\n",
            "remote: Total 2190 (delta 21), reused 2183 (delta 14), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (2190/2190), 5.15 MiB | 17.87 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzn0HgTUh5nS",
        "colab_type": "code",
        "outputId": "9cf6b071-4fc1-432e-d4c1-2ff92f37b287",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from farm.data_handler.data_silo import DataSilo\n",
        "from farm.data_handler.processor import TextClassificationProcessor\n",
        "from farm.modeling.optimization import initialize_optimizer\n",
        "from farm.infer import Inferencer\n",
        "from farm.modeling.adaptive_model import AdaptiveModel\n",
        "from farm.modeling.language_model import LanguageModel\n",
        "from farm.modeling.prediction_head import MultiLabelTextClassificationHead\n",
        "from farm.modeling.tokenization import Tokenizer\n",
        "from farm.train import Trainer\n",
        "from farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\n",
        "import logging\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "05/29/2020 09:05:03 - INFO - transformers.file_utils -   PyTorch version 1.4.0 available.\n",
            "05/29/2020 09:05:04 - INFO - transformers.file_utils -   TensorFlow version 2.2.0 available.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5WYnZKBrTZB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "e202171c-4e64-434c-92cc-272366e47132"
      },
      "source": [
        "# Farm allows simple logging of many parameters & metrics. Let's use the MLflow framework to track our experiment ...\n",
        "# You will see your results on https://public-mlflow.deepset.ai/\n",
        "\n",
        "ml_logger = MLFlowLogger(tracking_uri=\"https://public-mlflow.deepset.ai/\")\n",
        "ml_logger.init_experiment(experiment_name=\"BBC_Articles\", run_name=\"BBC News Articles\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " __          __  _                            _        \n",
            " \\ \\        / / | |                          | |       \n",
            "  \\ \\  /\\  / /__| | ___ ___  _ __ ___   ___  | |_ ___  \n",
            "   \\ \\/  \\/ / _ \\ |/ __/ _ \\| '_ ` _ \\ / _ \\ | __/ _ \\ \n",
            "    \\  /\\  /  __/ | (_| (_) | | | | | |  __/ | || (_) |\n",
            "     \\/  \\/ \\___|_|\\___\\___/|_| |_| |_|\\___|  \\__\\___/ \n",
            "  ______      _____  __  __  \n",
            " |  ____/\\   |  __ \\|  \\/  |              _.-^-._    .--.\n",
            " | |__ /  \\  | |__) | \\  / |           .-'   _   '-. |__|\n",
            " |  __/ /\\ \\ |  _  /| |\\/| |          /     |_|     \\|  |\n",
            " | | / ____ \\| | \\ \\| |  | |         /               \\  |\n",
            " |_|/_/    \\_\\_|  \\_\\_|  |_|        /|     _____     |\\ |\n",
            "                                     |    |==|==|    |  |\n",
            "|---||---|---|---|---|---|---|---|---|    |--|--|    |  |\n",
            "|---||---|---|---|---|---|---|---|---|    |==|==|    |  |\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqw3U4sQhysv",
        "colab_type": "code",
        "outputId": "2cddfae7-3481-46e6-c158-6e550bb7cf08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "set_all_seeds(seed=42)\n",
        "device, n_gpu = initialize_device_settings(use_cuda=True)\n",
        "n_epochs = 2\n",
        "batch_size = 8\n",
        "evaluate_every = 100"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "05/29/2020 09:05:07 - INFO - farm.utils -   device: cuda n_gpu: 1, distributed training: False, automatic mixed precision training: None\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPBEbzqi8Fe-",
        "colab_type": "text"
      },
      "source": [
        "## Building own blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fcaYbBYh_dv",
        "colab_type": "text"
      },
      "source": [
        "### Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZHgiGiGiBup",
        "colab_type": "code",
        "outputId": "1d6785b3-575f-4a2c-8dcc-c2b8b4a89a6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "f909fbb6e3434296b75ff3c18ce62158",
            "f1d88c151fd948d291ec5a5e3c91ee09",
            "454dce4b4f59404299b46a26aed371f9",
            "6cc377dd61db4e1d990ae24bdf322a36",
            "f05a6de7d93446e494eff8d0af4f4873",
            "e0827f6534fa45b98d76a8eac523f93a",
            "32d94e87764642048f472bcf072e3d61",
            "6a0078355610429381d731c529306322"
          ]
        }
      },
      "source": [
        "lang_model = \"bert-base-cased\"\n",
        "do_lower_case = False\n",
        "\n",
        "tokenizer = Tokenizer.load(\n",
        "    pretrained_model_name_or_path=lang_model,\n",
        "    do_lower_case=do_lower_case)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "05/29/2020 09:05:07 - INFO - farm.modeling.tokenization -   Loading tokenizer of type 'BertTokenizer'\n",
            "05/29/2020 09:05:07 - INFO - filelock -   Lock 140458994706136 acquired on /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1.lock\n",
            "05/29/2020 09:05:07 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp469ope3r\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f909fbb6e3434296b75ff3c18ce62158",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "05/29/2020 09:05:07 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt in cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
            "05/29/2020 09:05:07 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
            "05/29/2020 09:05:07 - INFO - filelock -   Lock 140458994706136 released on /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1.lock\n",
            "05/29/2020 09:05:07 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU3jLTR6iXfG",
        "colab_type": "text"
      },
      "source": [
        "### Data Processor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sChJb0tit8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_list = ['entertainment', 'sport', 'politics', 'business', 'tech'] #labels in our data set\n",
        "metric = \"f1_macro\" # desired metric for evaluation\n",
        "\n",
        "processor = TextClassificationProcessor(tokenizer=tokenizer,\n",
        "                                            max_seq_len=512, # BERT can only handle sequence lengths of up to 512\n",
        "                                            data_dir='bbc_news/generated_data', \n",
        "                                            label_list=label_list,\n",
        "                                            label_column_name=\"genre\", # our labels are located in the \"genre\" column\n",
        "                                            metric=metric,\n",
        "                                            quote_char='\"',\n",
        "                                            multilabel=True,\n",
        "                                            train_filename=\"train.tsv\",\n",
        "                                            dev_filename=None,\n",
        "                                            test_filename=\"test.tsv\",\n",
        "                                            dev_split=0.1 # this will extract 10% of the train set to create a dev set\n",
        "                                            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgeFkaddi5P3",
        "colab_type": "code",
        "outputId": "d279fa30-662e-443c-f835-7805018ab04a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "data_silo = DataSilo(\n",
        "    processor=processor,\n",
        "    batch_size=batch_size)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "05/29/2020 09:05:08 - INFO - farm.data_handler.data_silo -   \n",
            "Loading data into the data silo ... \n",
            "              ______\n",
            "               |o  |   !\n",
            "   __          |:`_|---'-.\n",
            "  |__|______.-/ _ \\-----.|       \n",
            " (o)(o)------'\\ _ /     ( )      \n",
            " \n",
            "05/29/2020 09:05:08 - INFO - farm.data_handler.data_silo -   Loading train set from: bbc_news/generated_data/train.tsv \n",
            "05/29/2020 09:05:08 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 1780 dictionaries to pytorch datasets (chunksize = 356)...\n",
            "05/29/2020 09:05:08 - INFO - farm.data_handler.data_silo -    0 \n",
            "05/29/2020 09:05:08 - INFO - farm.data_handler.data_silo -   /w\\\n",
            "05/29/2020 09:05:08 - INFO - farm.data_handler.data_silo -   /'\\\n",
            "05/29/2020 09:05:08 - INFO - farm.data_handler.data_silo -   \n",
            "Preprocessing Dataset bbc_news/generated_data/train.tsv:   0%|          | 0/1780 [00:00<?, ? Dicts/s]05/29/2020 09:05:13 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "05/29/2020 09:05:13 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-76-0\n",
            "Clear Text: \n",
            " \ttext: Home Secretary Charles Clarke has been quoted as telling Labour members he wants more migrants to come to the UK. Tory co-chairman Liam Fox said the comments were at odds with Tony Blair's prediction of a net cut in immigration. But Mr Clarke accused him of trying to score \"cheap political points\" by muddling immigration with asylum. London's Evening Standard quoted Mr Clarke telling Labour activists at a question and answer session in Gateshead that he wanted Britain to offer refuge for those fleeing tyranny. \"That's not only a moral duty and a legal duty, but something which is part of the essence of this country,\" he said. \"We want more migration, more people come to study and to work. \"We want more people coming to look for refuge.\" Mr Blair's was asked last Wednesday if the government's new immigration plans, including a point system for economic migrants, would reduce net migration. The prime minister told MPs: \"The abusers will be weeded out, and as a result of the end of chain migration [where families have an automatic right to settle], the numbers will probably fall.\" On Monday, Dr Fox told reporters: \"The prime minister has broken his word so many times in the past but now his promises do not even last a week. \"The Labour Party election pledges, even when they are so incredibly vague, do not even last four days.\" The Tories want quotas for economic migrants and refugees and on Tuesday will outline more details of their plans for health checks on migrants. Mr Clarke dismissed the latest Tory attack. \"This is simply a scurrilous attempt by the Tories to score cheap political points,\" he said. \"The Tories are purposely mixing together two separate issues of immigration and asylum.\" Mr Clarke said he had made clear the UK would welcome genuine economic migrants for key jobs on a strict points based system. And only asylum seekers genuinely fleeing death or persecution would be admitted. \"Under our plans we expect unfounded applications to continue to fall,\" he added. Earlier, Dr Fox accused Mr Blair and other Cabinet ministers of telling lies about Tory policies and then attacking the lies. He told BBC Radio: \"If you are willing to lie about the reasons for going to war, I guess you are going to lie about anything at all.\" The latest pre-election spats come after Mr Blair told Labour members the Tories offered a \"hard right agenda\" which would take Britain backwards. Lib Dem leader Charles Kennedy accelerating Lib Dem election preparations this week as he visits Manchester, Liverpool, Leicester, Somerset, Basingstoke, Shrewsbury, Dorset and Torbay.\n",
            " \ttext_classification_label: politics\n",
            "Tokenized: \n",
            " \ttokens: ['Home', 'Secretary', 'Charles', 'Clarke', 'has', 'been', 'quoted', 'as', 'telling', 'Labour', 'members', 'he', 'wants', 'more', 'migrants', 'to', 'come', 'to', 'the', 'UK', '.', 'Tory', 'co', '-', 'chairman', 'Liam', 'Fox', 'said', 'the', 'comments', 'were', 'at', 'odds', 'with', 'Tony', 'Blair', \"'\", 's', 'prediction', 'of', 'a', 'net', 'cut', 'in', 'immigration', '.', 'But', 'Mr', 'Clarke', 'accused', 'him', 'of', 'trying', 'to', 'score', '\"', 'cheap', 'political', 'points', '\"', 'by', 'mud', '##dling', 'immigration', 'with', 'asylum', '.', 'London', \"'\", 's', 'Evening', 'Standard', 'quoted', 'Mr', 'Clarke', 'telling', 'Labour', 'activists', 'at', 'a', 'question', 'and', 'answer', 'session', 'in', 'Gates', '##head', 'that', 'he', 'wanted', 'Britain', 'to', 'offer', 'refuge', 'for', 'those', 'fleeing', 't', '##yra', '##nny', '.', '\"', 'That', \"'\", 's', 'not', 'only', 'a', 'moral', 'duty', 'and', 'a', 'legal', 'duty', ',', 'but', 'something', 'which', 'is', 'part', 'of', 'the', 'essence', 'of', 'this', 'country', ',', '\"', 'he', 'said', '.', '\"', 'We', 'want', 'more', 'migration', ',', 'more', 'people', 'come', 'to', 'study', 'and', 'to', 'work', '.', '\"', 'We', 'want', 'more', 'people', 'coming', 'to', 'look', 'for', 'refuge', '.', '\"', 'Mr', 'Blair', \"'\", 's', 'was', 'asked', 'last', 'Wednesday', 'if', 'the', 'government', \"'\", 's', 'new', 'immigration', 'plans', ',', 'including', 'a', 'point', 'system', 'for', 'economic', 'migrants', ',', 'would', 'reduce', 'net', 'migration', '.', 'The', 'prime', 'minister', 'told', 'MPs', ':', '\"', 'The', 'abuse', '##rs', 'will', 'be', 'weed', '##ed', 'out', ',', 'and', 'as', 'a', 'result', 'of', 'the', 'end', 'of', 'chain', 'migration', '[', 'where', 'families', 'have', 'an', 'automatic', 'right', 'to', 'settle', ']', ',', 'the', 'numbers', 'will', 'probably', 'fall', '.', '\"', 'On', 'Monday', ',', 'Dr', 'Fox', 'told', 'reporters', ':', '\"', 'The', 'prime', 'minister', 'has', 'broken', 'his', 'word', 'so', 'many', 'times', 'in', 'the', 'past', 'but', 'now', 'his', 'promises', 'do', 'not', 'even', 'last', 'a', 'week', '.', '\"', 'The', 'Labour', 'Party', 'election', 'pledge', '##s', ',', 'even', 'when', 'they', 'are', 'so', 'incredibly', 'vague', ',', 'do', 'not', 'even', 'last', 'four', 'days', '.', '\"', 'The', 'Tori', '##es', 'want', 'quota', '##s', 'for', 'economic', 'migrants', 'and', 'refugees', 'and', 'on', 'Tuesday', 'will', 'outline', 'more', 'details', 'of', 'their', 'plans', 'for', 'health', 'checks', 'on', 'migrants', '.', 'Mr', 'Clarke', 'dismissed', 'the', 'latest', 'Tory', 'attack', '.', '\"', 'This', 'is', 'simply', 'a', 's', '##cu', '##rri', '##lous', 'attempt', 'by', 'the', 'Tori', '##es', 'to', 'score', 'cheap', 'political', 'points', ',', '\"', 'he', 'said', '.', '\"', 'The', 'Tori', '##es', 'are', 'purpose', '##ly', 'mixing', 'together', 'two', 'separate', 'issues', 'of', 'immigration', 'and', 'asylum', '.', '\"', 'Mr', 'Clarke', 'said', 'he', 'had', 'made', 'clear', 'the', 'UK', 'would', 'welcome', 'genuine', 'economic', 'migrants', 'for', 'key', 'jobs', 'on', 'a', 'strict', 'points', 'based', 'system', '.', 'And', 'only', 'asylum', 'seek', '##ers', 'genuinely', 'fleeing', 'death', 'or', 'persecution', 'would', 'be', 'admitted', '.', '\"', 'Under', 'our', 'plans', 'we', 'expect', 'un', '##founded', 'applications', 'to', 'continue', 'to', 'fall', ',', '\"', 'he', 'added', '.', 'Earlier', ',', 'Dr', 'Fox', 'accused', 'Mr', 'Blair', 'and', 'other', 'Cabinet', 'ministers', 'of', 'telling', 'lies', 'about', 'Tory', 'policies', 'and', 'then', 'attacking', 'the', 'lies', '.', 'He', 'told', 'BBC', 'Radio', ':', '\"', 'If', 'you', 'are', 'willing', 'to', 'lie', 'about', 'the', 'reasons', 'for', 'going', 'to', 'war', ',', 'I', 'guess', 'you', 'are', 'going', 'to', 'lie', 'about', 'anything', 'at', 'all', '.', '\"', 'The', 'latest', 'pre', '-', 'election', 'spat', '##s', 'come', 'after', 'Mr', 'Blair', 'told', 'Labour', 'members', 'the', 'Tori', '##es', 'offered', 'a', '\"', 'hard', 'right', 'agenda', '\"', 'which', 'would', 'take', 'Britain', 'backwards', '.', 'Li', '##b']\n",
            " \toffsets: [0, 5, 15, 23, 30, 34, 39, 46, 49, 57, 64, 72, 75, 81, 86, 95, 98, 103, 106, 110, 112, 114, 119, 121, 122, 131, 136, 140, 145, 149, 158, 163, 166, 171, 176, 181, 186, 187, 189, 200, 203, 205, 209, 213, 216, 227, 229, 233, 236, 243, 251, 255, 258, 265, 268, 274, 275, 281, 291, 297, 299, 302, 305, 311, 323, 328, 334, 336, 342, 343, 345, 353, 362, 369, 372, 379, 387, 394, 404, 407, 409, 418, 422, 429, 437, 440, 445, 450, 455, 458, 465, 473, 476, 482, 489, 493, 499, 507, 508, 511, 514, 516, 517, 521, 522, 524, 528, 533, 535, 541, 546, 550, 552, 558, 562, 564, 568, 578, 584, 587, 592, 595, 599, 607, 610, 615, 622, 623, 625, 628, 632, 634, 635, 638, 643, 648, 657, 659, 664, 671, 676, 679, 685, 689, 692, 696, 698, 699, 702, 707, 712, 719, 726, 729, 734, 738, 744, 745, 747, 750, 755, 756, 758, 762, 768, 773, 783, 786, 790, 800, 801, 803, 807, 819, 824, 826, 836, 838, 844, 851, 855, 864, 872, 874, 880, 887, 891, 900, 902, 906, 912, 921, 926, 929, 931, 932, 936, 941, 944, 949, 952, 956, 959, 962, 964, 968, 971, 973, 980, 983, 987, 991, 994, 1000, 1010, 1011, 1017, 1026, 1031, 1034, 1044, 1050, 1053, 1059, 1060, 1062, 1066, 1074, 1079, 1088, 1092, 1093, 1095, 1098, 1104, 1106, 1109, 1113, 1118, 1127, 1129, 1130, 1134, 1140, 1149, 1153, 1160, 1164, 1169, 1172, 1177, 1183, 1186, 1190, 1195, 1199, 1203, 1207, 1216, 1219, 1223, 1228, 1233, 1235, 1239, 1241, 1242, 1246, 1253, 1259, 1268, 1274, 1275, 1277, 1282, 1287, 1292, 1296, 1299, 1310, 1315, 1317, 1320, 1324, 1329, 1334, 1339, 1343, 1344, 1346, 1350, 1354, 1357, 1362, 1367, 1369, 1373, 1382, 1391, 1395, 1404, 1408, 1411, 1419, 1424, 1432, 1437, 1445, 1448, 1454, 1460, 1464, 1471, 1478, 1481, 1489, 1491, 1494, 1501, 1511, 1515, 1522, 1527, 1533, 1535, 1536, 1541, 1544, 1551, 1553, 1554, 1556, 1559, 1564, 1572, 1575, 1579, 1583, 1586, 1589, 1595, 1601, 1611, 1617, 1618, 1620, 1623, 1627, 1629, 1630, 1634, 1638, 1641, 1645, 1652, 1655, 1662, 1671, 1675, 1684, 1691, 1694, 1706, 1710, 1716, 1717, 1719, 1722, 1729, 1734, 1737, 1741, 1746, 1752, 1756, 1759, 1765, 1773, 1781, 1790, 1799, 1803, 1807, 1812, 1815, 1817, 1824, 1831, 1837, 1843, 1845, 1849, 1854, 1861, 1865, 1869, 1879, 1887, 1893, 1896, 1908, 1914, 1917, 1925, 1927, 1928, 1934, 1938, 1944, 1947, 1954, 1956, 1964, 1977, 1980, 1989, 1992, 1996, 1997, 1999, 2002, 2007, 2009, 2016, 2018, 2021, 2025, 2033, 2036, 2042, 2046, 2052, 2060, 2070, 2073, 2081, 2086, 2092, 2097, 2106, 2110, 2115, 2125, 2129, 2133, 2135, 2138, 2143, 2147, 2152, 2154, 2155, 2158, 2162, 2166, 2174, 2177, 2181, 2187, 2191, 2199, 2203, 2209, 2212, 2215, 2217, 2219, 2225, 2229, 2233, 2239, 2242, 2246, 2252, 2261, 2264, 2267, 2268, 2270, 2274, 2281, 2284, 2285, 2294, 2298, 2300, 2305, 2311, 2314, 2320, 2325, 2332, 2340, 2344, 2348, 2351, 2359, 2361, 2362, 2367, 2373, 2379, 2381, 2387, 2393, 2398, 2406, 2415, 2417, 2419]\n",
            " \tstart_of_word: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, False, True, True, True, False, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, False, False, True, False, False, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, False, True, False, True, True, True, False, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, False, False, True, True, False, False, True, True, True, True, True, True, True, False, False, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, False, True, False, True, False, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, False, True, True, True, True, True, False, False, True, True, False, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, False, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, False, True, True, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, False, True, True, True, True, False, False, False, True, True, True, True, False, True, True, True, True, True, False, False, True, True, False, True, False, True, False, True, True, False, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, False, True, True, True, True, True, False, False, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, False, False, True, False, True, True, True, True, True, True, True, Tru\n",
            "THE REST IS TOO LONG TO DISPLAY. Remaining chars: 109\n",
            "Features: \n",
            " \tinput_ids: [101, 3341, 2909, 1889, 7949, 1144, 1151, 9129, 1112, 3344, 4560, 1484, 1119, 3349, 1167, 18677, 1106, 1435, 1106, 1103, 1993, 119, 18377, 1884, 118, 3931, 7797, 3977, 1163, 1103, 7640, 1127, 1120, 10653, 1114, 3270, 10378, 112, 188, 20770, 1104, 170, 5795, 2195, 1107, 9027, 119, 1252, 1828, 7949, 4806, 1140, 1104, 1774, 1106, 2794, 107, 10928, 1741, 1827, 107, 1118, 9052, 21869, 9027, 1114, 15345, 119, 1498, 112, 188, 11718, 6433, 9129, 1828, 7949, 3344, 4560, 10254, 1120, 170, 2304, 1105, 2590, 4912, 1107, 12702, 3925, 1115, 1119, 1458, 2855, 1106, 2906, 10782, 1111, 1343, 14979, 189, 20153, 15863, 119, 107, 1337, 112, 188, 1136, 1178, 170, 7279, 4019, 1105, 170, 2732, 4019, 117, 1133, 1380, 1134, 1110, 1226, 1104, 1103, 12661, 1104, 1142, 1583, 117, 107, 1119, 1163, 119, 107, 1284, 1328, 1167, 10348, 117, 1167, 1234, 1435, 1106, 2025, 1105, 1106, 1250, 119, 107, 1284, 1328, 1167, 1234, 1909, 1106, 1440, 1111, 10782, 119, 107, 1828, 10378, 112, 188, 1108, 1455, 1314, 9031, 1191, 1103, 1433, 112, 188, 1207, 9027, 2714, 117, 1259, 170, 1553, 1449, 1111, 2670, 18677, 117, 1156, 4851, 5795, 10348, 119, 1109, 5748, 3907, 1500, 13904, 131, 107, 1109, 6704, 1733, 1209, 1129, 24195, 1174, 1149, 117, 1105, 1112, 170, 1871, 1104, 1103, 1322, 1104, 4129, 10348, 164, 1187, 2073, 1138, 1126, 6973, 1268, 1106, 7098, 166, 117, 1103, 2849, 1209, 1930, 2303, 119, 107, 1212, 6356, 117, 1987, 3977, 1500, 13509, 131, 107, 1109, 5748, 3907, 1144, 3088, 1117, 1937, 1177, 1242, 1551, 1107, 1103, 1763, 1133, 1208, 1117, 11323, 1202, 1136, 1256, 1314, 170, 1989, 119, 107, 1109, 4560, 1786, 1728, 20335, 1116, 117, 1256, 1165, 1152, 1132, 1177, 12170, 14673, 117, 1202, 1136, 1256, 1314, 1300, 1552, 119, 107, 1109, 28017, 1279, 1328, 23690, 1116, 1111, 2670, 18677, 1105, 8940, 1105, 1113, 9667, 1209, 13905, 1167, 4068, 1104, 1147, 2714, 1111, 2332, 15008, 1113, 18677, 119, 1828, 7949, 6714, 1103, 6270, 18377, 2035, 119, 107, 1188, 1110, 2566, 170, 188, 10182, 14791, 12769, 2661, 1118, 1103, 28017, 1279, 1106, 2794, 10928, 1741, 1827, 117, 107, 1119, 1163, 119, 107, 1109, 28017, 1279, 1132, 3007, 1193, 7021, 1487, 1160, 2767, 2492, 1104, 9027, 1105, 15345, 119, 107, 1828, 7949, 1163, 1119, 1125, 1189, 2330, 1103, 1993, 1156, 7236, 10416, 2670, 18677, 1111, 2501, 5448, 1113, 170, 9382, 1827, 1359, 1449, 119, 1262, 1178, 15345, 5622, 1468, 16997, 14979, 1473, 1137, 15873, 1156, 1129, 4120, 119, 107, 2831, 1412, 2714, 1195, 5363, 8362, 20897, 4683, 1106, 2760, 1106, 2303, 117, 107, 1119, 1896, 119, 15993, 117, 1987, 3977, 4806, 1828, 10378, 1105, 1168, 9049, 9813, 1104, 3344, 2887, 1164, 18377, 5502, 1105, 1173, 7492, 1103, 2887, 119, 1124, 1500, 3173, 2664, 131, 107, 1409, 1128, 1132, 4988, 1106, 4277, 1164, 1103, 3672, 1111, 1280, 1106, 1594, 117, 146, 3319, 1128, 1132, 1280, 1106, 4277, 1164, 1625, 1120, 1155, 119, 107, 1109, 6270, 3073, 118, 1728, 15732, 1116, 1435, 1170, 1828, 10378, 1500, 4560, 1484, 1103, 28017, 1279, 2356, 170, 107, 1662, 1268, 12932, 107, 1134, 1156, 1321, 2855, 11316, 119, 5255, 1830, 102]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0, 0, 1, 0, 0]\n",
            "_____________________________________________________\n",
            "05/29/2020 09:05:13 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-282-0\n",
            "Clear Text: \n",
            " \ttext: Tory spokesman David Davis told MPs the UK was losing its power of veto over who was allowed to come to Britain. The EU has opted to adopt qualified majority voting in this area - previously measures needed unanimous agreement from all member states. Mr Blunkett told MPs the UK would still be able to reject proposals on immigration it did not agree with. He argued closer co-operation with Europe over asylum and immigration was crucial to controlling the flow of people into the UK. \"If we don't like what other EU countries do on immigration and nationality we have the right to opt-in or out to suit the British people,\" he said. The home secretary was responding to an emergency question from his Tory opposite number Mr Davis. \"The government is employing the politics of confusion - I think, deliberately,\" argued Mr Davis. \"By confusing the country it hopes no one will notice the disappearance of the asylum and immigration veto.\" On Monday Tony Blair insisted closer co-operation did not mean losing control of British borders. He said an enlarged 25-member EU needs a streamlined decision making process. Mr Davis said once Britain had opted into policies then it could not opt out - leaving the UK open to unfavourable interpretations of what those policies actually meant. He accused the government of being \"pathetic\" when it came to its efforts over immigration and asylum and of \"surrendering\" on the issue. And he asked why the government was agreeing to the measure on asylum and immigration now when the whole issue was part of the EU constitution, which voters in the UK had been promised a referendum over. Mr Blair told his monthly news conference Britain had the \"best of both worlds\" and would keep the veto. \"There is no question of Britain giving up our veto on our border controls,\" he said. \"With the Treaty of Amsterdam seven years ago, we secured the absolute right to opt in to any of the asylum and immigration provisions that we wanted to in Europe.\" Mr Blunkett met his EU counterparts in Luxembourg on Monday to consider proposals aimed at streamlining decision making on the issue. The 25 member-states are expected to scrap the requirement for unanimous agreement on immigration policy, in favour of the qualified majority voting (QMV) system. Under this scheme larger states such as Britain are expected to have more power than the smaller EU states. Britain is expected to retain an \"opt-in\" right which will allow it to ignore any measures it disagrees with. Liberal Democrat spokesman Mark Oaten called the change pragmatic and argued it gave a better chance of producing a European asylum solution. \"If we don't work together it means some countries can ignore their responsibilities at the expense of their neighbours,\" said Mr Oaten. \"The Liberal Democrats have long argued that Britain should be a safe haven for asylum seekers but it's right that we don't do this in isolation.\"\n",
            " \ttext_classification_label: politics\n",
            "Tokenized: \n",
            " \ttokens: ['Tory', 'spokesman', 'David', 'Davis', 'told', 'MPs', 'the', 'UK', 'was', 'losing', 'its', 'power', 'of', 'veto', 'over', 'who', 'was', 'allowed', 'to', 'come', 'to', 'Britain', '.', 'The', 'EU', 'has', 'opted', 'to', 'adopt', 'qualified', 'majority', 'voting', 'in', 'this', 'area', '-', 'previously', 'measures', 'needed', 'unanimous', 'agreement', 'from', 'all', 'member', 'states', '.', 'Mr', 'Blu', '##nk', '##ett', 'told', 'MPs', 'the', 'UK', 'would', 'still', 'be', 'able', 'to', 'reject', 'proposals', 'on', 'immigration', 'it', 'did', 'not', 'agree', 'with', '.', 'He', 'argued', 'closer', 'co', '-', 'operation', 'with', 'Europe', 'over', 'asylum', 'and', 'immigration', 'was', 'crucial', 'to', 'controlling', 'the', 'flow', 'of', 'people', 'into', 'the', 'UK', '.', '\"', 'If', 'we', 'don', \"'\", 't', 'like', 'what', 'other', 'EU', 'countries', 'do', 'on', 'immigration', 'and', 'nationality', 'we', 'have', 'the', 'right', 'to', 'op', '##t', '-', 'in', 'or', 'out', 'to', 'suit', 'the', 'British', 'people', ',', '\"', 'he', 'said', '.', 'The', 'home', 'secretary', 'was', 'responding', 'to', 'an', 'emergency', 'question', 'from', 'his', 'Tory', 'opposite', 'number', 'Mr', 'Davis', '.', '\"', 'The', 'government', 'is', 'employing', 'the', 'politics', 'of', 'confusion', '-', 'I', 'think', ',', 'deliberately', ',', '\"', 'argued', 'Mr', 'Davis', '.', '\"', 'By', 'confusing', 'the', 'country', 'it', 'hopes', 'no', 'one', 'will', 'notice', 'the', 'disappearance', 'of', 'the', 'asylum', 'and', 'immigration', 'veto', '.', '\"', 'On', 'Monday', 'Tony', 'Blair', 'insisted', 'closer', 'co', '-', 'operation', 'did', 'not', 'mean', 'losing', 'control', 'of', 'British', 'borders', '.', 'He', 'said', 'an', 'enlarged', '25', '-', 'member', 'EU', 'needs', 'a', 'stream', '##lined', 'decision', 'making', 'process', '.', 'Mr', 'Davis', 'said', 'once', 'Britain', 'had', 'opted', 'into', 'policies', 'then', 'it', 'could', 'not', 'op', '##t', 'out', '-', 'leaving', 'the', 'UK', 'open', 'to', 'un', '##fa', '##vour', '##able', 'interpretations', 'of', 'what', 'those', 'policies', 'actually', 'meant', '.', 'He', 'accused', 'the', 'government', 'of', 'being', '\"', 'pathetic', '\"', 'when', 'it', 'came', 'to', 'its', 'efforts', 'over', 'immigration', 'and', 'asylum', 'and', 'of', '\"', 'surrender', '##ing', '\"', 'on', 'the', 'issue', '.', 'And', 'he', 'asked', 'why', 'the', 'government', 'was', 'agreeing', 'to', 'the', 'measure', 'on', 'asylum', 'and', 'immigration', 'now', 'when', 'the', 'whole', 'issue', 'was', 'part', 'of', 'the', 'EU', 'constitution', ',', 'which', 'voters', 'in', 'the', 'UK', 'had', 'been', 'promised', 'a', 'referendum', 'over', '.', 'Mr', 'Blair', 'told', 'his', 'monthly', 'news', 'conference', 'Britain', 'had', 'the', '\"', 'best', 'of', 'both', 'worlds', '\"', 'and', 'would', 'keep', 'the', 'veto', '.', '\"', 'There', 'is', 'no', 'question', 'of', 'Britain', 'giving', 'up', 'our', 'veto', 'on', 'our', 'border', 'controls', ',', '\"', 'he', 'said', '.', '\"', 'With', 'the', 'Treaty', 'of', 'Amsterdam', 'seven', 'years', 'ago', ',', 'we', 'secured', 'the', 'absolute', 'right', 'to', 'op', '##t', 'in', 'to', 'any', 'of', 'the', 'asylum', 'and', 'immigration', 'provisions', 'that', 'we', 'wanted', 'to', 'in', 'Europe', '.', '\"', 'Mr', 'Blu', '##nk', '##ett', 'met', 'his', 'EU', 'counterparts', 'in', 'Luxembourg', 'on', 'Monday', 'to', 'consider', 'proposals', 'aimed', 'at', 'stream', '##lining', 'decision', 'making', 'on', 'the', 'issue', '.', 'The', '25', 'member', '-', 'states', 'are', 'expected', 'to', 'scrap', 'the', 'requirement', 'for', 'unanimous', 'agreement', 'on', 'immigration', 'policy', ',', 'in', 'favour', 'of', 'the', 'qualified', 'majority', 'voting', '(', 'Q', '##M', '##V', ')', 'system', '.', 'Under', 'this', 'scheme', 'larger', 'states', 'such', 'as', 'Britain', 'are', 'expected', 'to', 'have', 'more', 'power', 'than', 'the', 'smaller', 'EU', 'states', '.', 'Britain', 'is', 'expected', 'to', 'retain', 'an', '\"', 'op', '##t', '-', 'in', '\"', 'right', 'which', 'will', 'allow', 'it', 'to', 'ignore', 'any', 'measures', 'it', 'disagree', '##s', 'with', '.', 'Liberal', 'Democrat', 'spokesman', 'Mark', 'O', '##ate']\n",
            " \toffsets: [0, 5, 15, 21, 27, 32, 36, 40, 43, 47, 54, 58, 64, 67, 72, 77, 81, 85, 93, 96, 101, 104, 111, 113, 117, 120, 124, 130, 133, 139, 149, 158, 165, 168, 173, 178, 180, 191, 200, 207, 217, 227, 232, 236, 243, 249, 251, 254, 257, 259, 263, 268, 272, 276, 279, 285, 291, 294, 299, 302, 309, 319, 322, 334, 337, 341, 345, 351, 355, 357, 360, 367, 374, 376, 377, 387, 392, 399, 404, 411, 415, 427, 431, 439, 442, 454, 458, 463, 466, 473, 478, 482, 484, 486, 487, 490, 493, 496, 497, 499, 504, 509, 515, 518, 528, 531, 534, 546, 550, 562, 565, 570, 574, 580, 583, 585, 586, 587, 590, 593, 597, 600, 605, 609, 617, 623, 624, 626, 629, 633, 635, 639, 644, 654, 658, 669, 672, 675, 685, 694, 699, 703, 708, 717, 724, 727, 732, 734, 735, 739, 750, 753, 763, 767, 776, 779, 789, 791, 793, 798, 800, 812, 813, 815, 822, 825, 830, 832, 833, 836, 846, 850, 858, 861, 867, 870, 874, 879, 886, 890, 904, 907, 911, 918, 922, 934, 938, 939, 941, 944, 951, 956, 962, 971, 978, 980, 981, 991, 995, 999, 1004, 1011, 1019, 1022, 1030, 1037, 1039, 1042, 1047, 1050, 1059, 1061, 1062, 1069, 1072, 1078, 1080, 1086, 1092, 1101, 1108, 1115, 1117, 1120, 1126, 1131, 1136, 1144, 1148, 1154, 1159, 1168, 1173, 1176, 1182, 1186, 1188, 1190, 1194, 1196, 1204, 1208, 1211, 1216, 1219, 1221, 1223, 1227, 1232, 1248, 1251, 1256, 1262, 1271, 1280, 1285, 1287, 1290, 1298, 1302, 1313, 1316, 1322, 1323, 1331, 1333, 1338, 1341, 1346, 1349, 1353, 1361, 1366, 1378, 1382, 1389, 1393, 1396, 1397, 1406, 1409, 1411, 1414, 1418, 1423, 1425, 1429, 1432, 1438, 1442, 1446, 1457, 1461, 1470, 1473, 1477, 1485, 1488, 1495, 1499, 1511, 1515, 1520, 1524, 1530, 1536, 1540, 1545, 1548, 1552, 1555, 1567, 1569, 1575, 1582, 1585, 1589, 1592, 1596, 1601, 1610, 1612, 1623, 1627, 1629, 1632, 1638, 1643, 1647, 1655, 1660, 1671, 1679, 1683, 1687, 1688, 1693, 1696, 1701, 1707, 1709, 1713, 1719, 1724, 1728, 1732, 1734, 1735, 1741, 1744, 1747, 1756, 1759, 1767, 1774, 1777, 1781, 1786, 1789, 1793, 1800, 1808, 1809, 1811, 1814, 1818, 1820, 1821, 1826, 1830, 1837, 1840, 1850, 1856, 1862, 1865, 1867, 1870, 1878, 1882, 1891, 1897, 1900, 1902, 1904, 1907, 1910, 1914, 1917, 1921, 1928, 1932, 1944, 1955, 1960, 1963, 1970, 1973, 1976, 1982, 1983, 1985, 1988, 1991, 1993, 1997, 2001, 2005, 2008, 2021, 2024, 2035, 2038, 2045, 2048, 2057, 2067, 2073, 2076, 2082, 2089, 2098, 2105, 2108, 2112, 2117, 2119, 2123, 2126, 2132, 2133, 2140, 2144, 2153, 2156, 2162, 2166, 2178, 2182, 2192, 2202, 2205, 2217, 2223, 2225, 2228, 2235, 2238, 2242, 2252, 2261, 2268, 2269, 2270, 2271, 2272, 2274, 2280, 2282, 2288, 2293, 2300, 2307, 2314, 2319, 2322, 2330, 2334, 2343, 2346, 2351, 2356, 2362, 2367, 2371, 2379, 2382, 2388, 2390, 2398, 2401, 2410, 2413, 2420, 2423, 2424, 2426, 2427, 2428, 2430, 2432, 2438, 2444, 2449, 2455, 2458, 2461, 2468, 2472, 2481, 2484, 2492, 2494, 2498, 2500, 2508, 2517, 2527, 2532, 2533]\n",
            " \tstart_of_word: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, True, True, True, True, True, True, True, False, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, False, True, False, False, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, False, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, False, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, False, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, False, False, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, Tr\n",
            "THE REST IS TOO LONG TO DISPLAY. Remaining chars: 197\n",
            "Features: \n",
            " \tinput_ids: [101, 18377, 15465, 1681, 3635, 1500, 13904, 1103, 1993, 1108, 3196, 1157, 1540, 1104, 27484, 1166, 1150, 1108, 2148, 1106, 1435, 1106, 2855, 119, 1109, 7270, 1144, 12243, 1106, 11258, 4452, 2656, 6612, 1107, 1142, 1298, 118, 2331, 5252, 1834, 14285, 3311, 1121, 1155, 1420, 2231, 119, 1828, 15223, 6773, 5912, 1500, 13904, 1103, 1993, 1156, 1253, 1129, 1682, 1106, 16589, 10698, 1113, 9027, 1122, 1225, 1136, 5340, 1114, 119, 1124, 4491, 2739, 1884, 118, 2805, 1114, 1980, 1166, 15345, 1105, 9027, 1108, 10268, 1106, 9783, 1103, 4235, 1104, 1234, 1154, 1103, 1993, 119, 107, 1409, 1195, 1274, 112, 189, 1176, 1184, 1168, 7270, 2182, 1202, 1113, 9027, 1105, 11242, 1195, 1138, 1103, 1268, 1106, 11769, 1204, 118, 1107, 1137, 1149, 1106, 4228, 1103, 1418, 1234, 117, 107, 1119, 1163, 119, 1109, 1313, 4848, 1108, 16322, 1106, 1126, 5241, 2304, 1121, 1117, 18377, 3714, 1295, 1828, 3635, 119, 107, 1109, 1433, 1110, 16846, 1103, 4039, 1104, 6406, 118, 146, 1341, 117, 9938, 117, 107, 4491, 1828, 3635, 119, 107, 1650, 18110, 1103, 1583, 1122, 7816, 1185, 1141, 1209, 4430, 1103, 14158, 1104, 1103, 15345, 1105, 9027, 27484, 119, 107, 1212, 6356, 3270, 10378, 6744, 2739, 1884, 118, 2805, 1225, 1136, 1928, 3196, 1654, 1104, 1418, 6641, 119, 1124, 1163, 1126, 12089, 1512, 118, 1420, 7270, 2993, 170, 5118, 14523, 2383, 1543, 1965, 119, 1828, 3635, 1163, 1517, 2855, 1125, 12243, 1154, 5502, 1173, 1122, 1180, 1136, 11769, 1204, 1149, 118, 2128, 1103, 1993, 1501, 1106, 8362, 8057, 17532, 1895, 17555, 1104, 1184, 1343, 5502, 2140, 2318, 119, 1124, 4806, 1103, 1433, 1104, 1217, 107, 18970, 107, 1165, 1122, 1338, 1106, 1157, 3268, 1166, 9027, 1105, 15345, 1105, 1104, 107, 7906, 1158, 107, 1113, 1103, 2486, 119, 1262, 1119, 1455, 1725, 1103, 1433, 1108, 17577, 1106, 1103, 4929, 1113, 15345, 1105, 9027, 1208, 1165, 1103, 2006, 2486, 1108, 1226, 1104, 1103, 7270, 7119, 117, 1134, 7179, 1107, 1103, 1993, 1125, 1151, 5163, 170, 9905, 1166, 119, 1828, 10378, 1500, 1117, 7868, 2371, 3511, 2855, 1125, 1103, 107, 1436, 1104, 1241, 11308, 107, 1105, 1156, 1712, 1103, 27484, 119, 107, 1247, 1110, 1185, 2304, 1104, 2855, 2368, 1146, 1412, 27484, 1113, 1412, 3070, 7451, 117, 107, 1119, 1163, 119, 107, 1556, 1103, 6599, 1104, 7101, 1978, 1201, 2403, 117, 1195, 6561, 1103, 7846, 1268, 1106, 11769, 1204, 1107, 1106, 1251, 1104, 1103, 15345, 1105, 9027, 8939, 1115, 1195, 1458, 1106, 1107, 1980, 119, 107, 1828, 15223, 6773, 5912, 1899, 1117, 7270, 15289, 1107, 10665, 1113, 6356, 1106, 4615, 10698, 5850, 1120, 5118, 13260, 2383, 1543, 1113, 1103, 2486, 119, 1109, 1512, 1420, 118, 2231, 1132, 2637, 1106, 16720, 1103, 8875, 1111, 14285, 3311, 1113, 9027, 2818, 117, 1107, 7511, 1104, 1103, 4452, 2656, 6612, 113, 154, 2107, 2559, 114, 1449, 119, 2831, 1142, 5471, 2610, 2231, 1216, 1112, 2855, 1132, 2637, 1106, 1138, 1167, 1540, 1190, 1103, 2964, 7270, 2231, 119, 2855, 1110, 2637, 1106, 8983, 1126, 107, 11769, 1204, 118, 1107, 107, 1268, 1134, 1209, 2621, 1122, 1106, 8429, 1251, 5252, 1122, 23423, 1116, 1114, 119, 4561, 7319, 15465, 2392, 152, 2193, 102]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0, 0, 1, 0, 0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset bbc_news/generated_data/train.tsv: 100%|██████████| 1780/1780 [00:25<00:00, 70.41 Dicts/s]\n",
            "05/29/2020 09:05:33 - INFO - farm.data_handler.data_silo -   Loading dev set as a slice of train set\n",
            "05/29/2020 09:05:33 - INFO - farm.data_handler.data_silo -   Took 356 samples out of train set to create dev set (dev split is roughly 0.1)\n",
            "05/29/2020 09:05:33 - INFO - farm.data_handler.data_silo -   Loading test set from: bbc_news/generated_data/test.tsv\n",
            "05/29/2020 09:05:33 - INFO - farm.data_handler.data_silo -   Got ya 1 parallel workers to convert 445 dictionaries to pytorch datasets (chunksize = 89)...\n",
            "05/29/2020 09:05:33 - INFO - farm.data_handler.data_silo -    0 \n",
            "05/29/2020 09:05:33 - INFO - farm.data_handler.data_silo -   /w\\\n",
            "05/29/2020 09:05:33 - INFO - farm.data_handler.data_silo -   / \\\n",
            "05/29/2020 09:05:33 - INFO - farm.data_handler.data_silo -   \n",
            "Preprocessing Dataset bbc_news/generated_data/test.tsv:   0%|          | 0/445 [00:00<?, ? Dicts/s]05/29/2020 09:05:34 - INFO - farm.data_handler.processor -   *** Show 2 random examples ***\n",
            "05/29/2020 09:05:34 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-37-0\n",
            "Clear Text: \n",
            " \ttext: The top seed was a strong favourite for the title but went down 7-6 (7-4) 6-3 to the American. Dent will face Juan Ignacio Chela next after the fourth seed was too strong for Jurgen Melzer. Olivier Rochus beat third seed Nicolas Kiefer 6-7 (4-7) 7-6 (8-6) 7-5 and will take on second seed Joachim Johansson. The Swede reached the last four by beating compatriot Thomas Enqvist 6-3 4-6 6-1. \"I felt like I was striking the ball much better,\" said Johansson. \"I felt like I had a lot of break chances, I didn't take care of them all, but I broke him four times and he only broke me once. \"I felt that was the key to get up in the set early.\"\n",
            " \ttext_classification_label: sport\n",
            "Tokenized: \n",
            " \ttokens: ['The', 'top', 'seed', 'was', 'a', 'strong', 'favourite', 'for', 'the', 'title', 'but', 'went', 'down', '7', '-', '6', '(', '7', '-', '4', ')', '6', '-', '3', 'to', 'the', 'American', '.', 'Den', '##t', 'will', 'face', 'Juan', 'Ignacio', 'Ch', '##ela', 'next', 'after', 'the', 'fourth', 'seed', 'was', 'too', 'strong', 'for', 'Ju', '##rgen', 'Mel', '##zer', '.', 'Olivier', 'R', '##och', '##us', 'beat', 'third', 'seed', 'Nicolas', 'Ki', '##ef', '##er', '6', '-', '7', '(', '4', '-', '7', ')', '7', '-', '6', '(', '8', '-', '6', ')', '7', '-', '5', 'and', 'will', 'take', 'on', 'second', 'seed', 'Joachim', 'Johan', '##sson', '.', 'The', 'S', '##wed', '##e', 'reached', 'the', 'last', 'four', 'by', 'beating', 'com', '##pa', '##tri', '##ot', 'Thomas', 'En', '##q', '##vist', '6', '-', '3', '4', '-', '6', '6', '-', '1', '.', '\"', 'I', 'felt', 'like', 'I', 'was', 'striking', 'the', 'ball', 'much', 'better', ',', '\"', 'said', 'Johan', '##sson', '.', '\"', 'I', 'felt', 'like', 'I', 'had', 'a', 'lot', 'of', 'break', 'chances', ',', 'I', 'didn', \"'\", 't', 'take', 'care', 'of', 'them', 'all', ',', 'but', 'I', 'broke', 'him', 'four', 'times', 'and', 'he', 'only', 'broke', 'me', 'once', '.', '\"', 'I', 'felt', 'that', 'was', 'the', 'key', 'to', 'get', 'up', 'in', 'the', 'set', 'early', '.', '\"']\n",
            " \toffsets: [0, 4, 8, 13, 17, 19, 26, 36, 40, 44, 50, 54, 59, 64, 65, 66, 68, 69, 70, 71, 72, 74, 75, 76, 78, 81, 85, 93, 95, 98, 100, 105, 110, 115, 123, 125, 129, 134, 140, 144, 151, 156, 160, 164, 171, 175, 177, 182, 185, 188, 190, 198, 199, 202, 205, 210, 216, 221, 229, 231, 233, 236, 237, 238, 240, 241, 242, 243, 244, 246, 247, 248, 250, 251, 252, 253, 254, 256, 257, 258, 260, 264, 269, 274, 277, 284, 289, 297, 302, 306, 308, 312, 313, 316, 318, 326, 330, 335, 340, 343, 351, 354, 356, 359, 362, 369, 371, 372, 377, 378, 379, 381, 382, 383, 385, 386, 387, 388, 390, 391, 393, 398, 403, 405, 409, 418, 422, 427, 432, 438, 439, 441, 446, 451, 455, 457, 458, 460, 465, 470, 472, 476, 478, 482, 485, 491, 498, 500, 502, 506, 507, 509, 514, 519, 522, 527, 530, 532, 536, 538, 544, 548, 553, 559, 563, 566, 571, 577, 580, 584, 586, 587, 589, 594, 599, 603, 607, 611, 614, 618, 621, 624, 628, 632, 637, 638]\n",
            " \tstart_of_word: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, False, False, False, False, True, False, False, True, True, True, False, True, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, False, False, True, True, False, False, True, True, True, True, True, False, False, True, False, False, True, False, False, False, False, True, False, False, True, False, False, False, False, True, False, False, True, True, True, True, True, True, True, True, False, False, True, True, False, False, True, True, True, True, True, True, True, False, False, False, True, True, False, False, True, False, False, True, False, False, True, False, False, False, True, False, True, True, True, True, True, True, True, True, True, False, False, True, True, False, False, True, False, True, True, True, True, True, True, True, True, True, False, True, True, False, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, False]\n",
            "Features: \n",
            " \tinput_ids: [101, 1109, 1499, 6478, 1108, 170, 2012, 9122, 1111, 1103, 1641, 1133, 1355, 1205, 128, 118, 127, 113, 128, 118, 125, 114, 127, 118, 124, 1106, 1103, 1237, 119, 14760, 1204, 1209, 1339, 4593, 25713, 20394, 9945, 1397, 1170, 1103, 2223, 6478, 1108, 1315, 2012, 1111, 23915, 16648, 11637, 6198, 119, 15061, 155, 9962, 1361, 3222, 1503, 6478, 10418, 14477, 11470, 1200, 127, 118, 128, 113, 125, 118, 128, 114, 128, 118, 127, 113, 129, 118, 127, 114, 128, 118, 126, 1105, 1209, 1321, 1113, 1248, 6478, 18473, 13402, 6598, 119, 1109, 156, 11547, 1162, 1680, 1103, 1314, 1300, 1118, 5405, 3254, 4163, 19091, 3329, 1819, 13832, 4426, 18295, 127, 118, 124, 125, 118, 127, 127, 118, 122, 119, 107, 146, 1464, 1176, 146, 1108, 8261, 1103, 3240, 1277, 1618, 117, 107, 1163, 13402, 6598, 119, 107, 146, 1464, 1176, 146, 1125, 170, 1974, 1104, 2549, 9820, 117, 146, 1238, 112, 189, 1321, 1920, 1104, 1172, 1155, 117, 1133, 146, 2795, 1140, 1300, 1551, 1105, 1119, 1178, 2795, 1143, 1517, 119, 107, 146, 1464, 1115, 1108, 1103, 2501, 1106, 1243, 1146, 1107, 1103, 1383, 1346, 119, 107, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [0, 1, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "05/29/2020 09:05:34 - INFO - farm.data_handler.processor -   \n",
            "\n",
            "      .--.        _____                       _      \n",
            "    .'_\\/_'.     / ____|                     | |     \n",
            "    '. /\\ .'    | (___   __ _ _ __ ___  _ __ | | ___ \n",
            "      \"||\"       \\___ \\ / _` | '_ ` _ \\| '_ \\| |/ _ \\ \n",
            "       || /\\     ____) | (_| | | | | | | |_) | |  __/\n",
            "    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n",
            "   (/\\||/                             |_|           \n",
            "______\\||/___________________________________________                     \n",
            "\n",
            "ID: train-14-0\n",
            "Clear Text: \n",
            " \ttext: The surge was led by animated movie Howl's Moving Castle, which took 20bn yen (Â£102m) to become the biggest film in Japan in 2004. It is expected to match the 30.7bn yen (Â£157m) record of Hayao Miyazaki's previous film Spirited Away. Japan Motion Picture Producers figures showed that 170 million cinema admissions were made in Japan in 2004. The Last Samurai, starring Tom Cruise, was the biggest foreign movie hit in Japan last year, taking 13.8bn yen (Â£70.7m). It was followed by Harry Potter and the Prisoner of Azkaban, Finding Nemo and The Lord of the Rings: The Return of the King. The second highest-grossing Japanese film was romantic drama Crying Out Love in the Centre of the World, followed by Be With You and Pocket Monsters Advanced Generation. Japanese films accounted for 37.5% of Japan's box office total last year, with foreign films taking the remaining 62.5%. This represented a 4.5% gain for the proportion of Japanese films in 2004 compared to 2003. The number of Japanese films released rose to 310 in 2004 from 287 the previous year. Sales of movies on DVD and video amounted to 497bn yen (Â£2.54bn) for the year.\n",
            " \ttext_classification_label: entertainment\n",
            "Tokenized: \n",
            " \ttokens: ['The', 'surge', 'was', 'led', 'by', 'animated', 'movie', 'How', '##l', \"'\", 's', 'Moving', 'Castle', ',', 'which', 'took', '20', '##b', '##n', 'ye', '##n', '(', 'Â', '##£', '##10', '##2', '##m', ')', 'to', 'become', 'the', 'biggest', 'film', 'in', 'Japan', 'in', '2004', '.', 'It', 'is', 'expected', 'to', 'match', 'the', '30', '.', '7', '##b', '##n', 'ye', '##n', '(', 'Â', '##£', '##15', '##7', '##m', ')', 'record', 'of', 'Hay', '##ao', 'Mi', '##ya', '##zaki', \"'\", 's', 'previous', 'film', 'Spirit', '##ed', 'Away', '.', 'Japan', 'Motion', 'Picture', 'Producers', 'figures', 'showed', 'that', '170', 'million', 'cinema', 'admissions', 'were', 'made', 'in', 'Japan', 'in', '2004', '.', 'The', 'Last', 'Sam', '##urai', ',', 'starring', 'Tom', 'Cruise', ',', 'was', 'the', 'biggest', 'foreign', 'movie', 'hit', 'in', 'Japan', 'last', 'year', ',', 'taking', '13', '.', '8', '##b', '##n', 'ye', '##n', '(', 'Â', '##£', '##70', '.', '7', '##m', ')', '.', 'It', 'was', 'followed', 'by', 'Harry', 'Potter', 'and', 'the', 'Prison', '##er', 'of', 'A', '##z', '##ka', '##ban', ',', 'Finding', 'N', '##em', '##o', 'and', 'The', 'Lord', 'of', 'the', 'Rings', ':', 'The', 'Return', 'of', 'the', 'King', '.', 'The', 'second', 'highest', '-', 'grossing', 'Japanese', 'film', 'was', 'romantic', 'drama', 'Cry', '##ing', 'Out', 'Love', 'in', 'the', 'Centre', 'of', 'the', 'World', ',', 'followed', 'by', 'Be', 'With', 'You', 'and', 'Pocket', 'Monsters', 'Advanced', 'Generation', '.', 'Japanese', 'films', 'accounted', 'for', '37', '.', '5', '%', 'of', 'Japan', \"'\", 's', 'box', 'office', 'total', 'last', 'year', ',', 'with', 'foreign', 'films', 'taking', 'the', 'remaining', '62', '.', '5', '%', '.', 'This', 'represented', 'a', '4', '.', '5', '%', 'gain', 'for', 'the', 'proportion', 'of', 'Japanese', 'films', 'in', '2004', 'compared', 'to', '2003', '.', 'The', 'number', 'of', 'Japanese', 'films', 'released', 'rose', 'to', '310', 'in', '2004', 'from', '287', 'the', 'previous', 'year', '.', 'Sales', 'of', 'movies', 'on', 'DVD', 'and', 'video', 'amounted', 'to', '49', '##7', '##b', '##n', 'ye', '##n', '(', 'Â', '##£', '##2', '.', '54', '##b', '##n', ')', 'for', 'the', 'year', '.']\n",
            " \toffsets: [0, 4, 10, 14, 18, 21, 30, 36, 39, 40, 41, 43, 50, 56, 58, 64, 69, 71, 72, 74, 76, 78, 79, 80, 81, 83, 84, 85, 87, 90, 97, 101, 109, 114, 117, 123, 126, 130, 132, 135, 138, 147, 150, 156, 160, 162, 163, 164, 165, 167, 169, 171, 172, 173, 174, 176, 177, 178, 180, 187, 190, 193, 196, 198, 200, 204, 205, 207, 216, 221, 227, 230, 234, 236, 242, 249, 257, 267, 275, 282, 287, 291, 299, 306, 317, 322, 327, 330, 336, 339, 343, 345, 349, 354, 357, 361, 363, 372, 376, 382, 384, 388, 392, 400, 408, 414, 418, 421, 427, 432, 436, 438, 445, 447, 448, 449, 450, 452, 454, 456, 457, 458, 459, 461, 462, 463, 464, 465, 467, 470, 474, 483, 486, 492, 499, 503, 507, 513, 516, 519, 520, 521, 523, 526, 528, 536, 537, 539, 541, 545, 549, 554, 557, 561, 566, 568, 572, 579, 582, 586, 590, 592, 596, 603, 610, 611, 620, 629, 634, 638, 647, 653, 656, 660, 664, 669, 672, 676, 683, 686, 690, 695, 697, 706, 709, 712, 717, 721, 725, 732, 741, 750, 760, 762, 771, 777, 787, 791, 793, 794, 795, 797, 800, 805, 806, 808, 812, 819, 825, 830, 834, 836, 841, 849, 855, 862, 866, 876, 878, 879, 880, 881, 883, 888, 900, 902, 903, 904, 905, 907, 912, 916, 920, 931, 934, 943, 949, 952, 957, 966, 969, 973, 975, 979, 986, 989, 998, 1004, 1013, 1018, 1021, 1025, 1028, 1033, 1038, 1042, 1046, 1055, 1059, 1061, 1067, 1070, 1077, 1080, 1084, 1088, 1094, 1103, 1106, 1108, 1109, 1110, 1112, 1114, 1116, 1117, 1118, 1119, 1120, 1121, 1123, 1124, 1125, 1127, 1131, 1135, 1139]\n",
            " \tstart_of_word: [True, True, True, True, True, True, True, True, False, False, False, True, True, False, True, True, True, False, False, True, False, True, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, False, False, False, True, False, True, False, False, False, False, False, False, True, True, True, False, True, False, False, False, False, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, False, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, False, False, False, False, True, False, True, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, False, True, True, False, False, False, False, True, True, False, False, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, False, False, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, False, False, True, True, False, False, True, True, True, True, True, False, True, True, True, True, True, True, True, False, False, False, False, True, True, True, True, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, False, False, False, True, False, True, False, False, False, False, False, False, False, False, True, True, True, False]\n",
            "Features: \n",
            " \tinput_ids: [101, 1109, 12814, 1108, 1521, 1118, 6608, 2523, 1731, 1233, 112, 188, 13091, 3856, 117, 1134, 1261, 1406, 1830, 1179, 6798, 1179, 113, 228, 28159, 10424, 1477, 1306, 114, 1106, 1561, 1103, 4583, 1273, 1107, 1999, 1107, 1516, 119, 1135, 1110, 2637, 1106, 1801, 1103, 1476, 119, 128, 1830, 1179, 6798, 1179, 113, 228, 28159, 16337, 1559, 1306, 114, 1647, 1104, 16164, 9513, 12107, 2315, 21150, 112, 188, 2166, 1273, 7258, 1174, 9252, 119, 1999, 12153, 10041, 24189, 3736, 2799, 1115, 10837, 1550, 7678, 25831, 1127, 1189, 1107, 1999, 1107, 1516, 119, 1109, 4254, 2687, 17319, 117, 3937, 2545, 16474, 117, 1108, 1103, 4583, 2880, 2523, 1855, 1107, 1999, 1314, 1214, 117, 1781, 1492, 119, 129, 1830, 1179, 6798, 1179, 113, 228, 28159, 20829, 119, 128, 1306, 114, 119, 1135, 1108, 1723, 1118, 3466, 11434, 1105, 1103, 11375, 1200, 1104, 138, 1584, 1968, 7167, 117, 18036, 151, 5521, 1186, 1105, 1109, 2188, 1104, 1103, 22518, 131, 1109, 11121, 1104, 1103, 1624, 119, 1109, 1248, 2439, 118, 19842, 1983, 1273, 1108, 6376, 3362, 15586, 1158, 3929, 2185, 1107, 1103, 2961, 1104, 1103, 1291, 117, 1723, 1118, 4108, 1556, 1192, 1105, 22595, 20971, 8445, 10617, 119, 1983, 2441, 15365, 1111, 3413, 119, 126, 110, 1104, 1999, 112, 188, 2884, 1701, 1703, 1314, 1214, 117, 1114, 2880, 2441, 1781, 1103, 2735, 5073, 119, 126, 110, 119, 1188, 2533, 170, 125, 119, 126, 110, 4361, 1111, 1103, 10807, 1104, 1983, 2441, 1107, 1516, 3402, 1106, 1581, 119, 1109, 1295, 1104, 1983, 2441, 1308, 3152, 1106, 18333, 1107, 1516, 1121, 25724, 1103, 2166, 1214, 119, 15689, 1104, 5558, 1113, 4173, 1105, 1888, 20317, 1106, 3927, 1559, 1830, 1179, 6798, 1179, 113, 228, 28159, 1477, 119, 4335, 1830, 1179, 114, 1111, 1103, 1214, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " \ttext_classification_label_ids: [1, 0, 0, 0, 0]\n",
            "_____________________________________________________\n",
            "Preprocessing Dataset bbc_news/generated_data/test.tsv: 100%|██████████| 445/445 [00:06<00:00, 64.86 Dicts/s]\n",
            "05/29/2020 09:05:40 - INFO - farm.data_handler.data_silo -   Examples in train: 1424\n",
            "05/29/2020 09:05:40 - INFO - farm.data_handler.data_silo -   Examples in dev  : 356\n",
            "05/29/2020 09:05:40 - INFO - farm.data_handler.data_silo -   Examples in test : 445\n",
            "05/29/2020 09:05:40 - INFO - farm.data_handler.data_silo -   \n",
            "05/29/2020 09:05:40 - INFO - farm.data_handler.data_silo -   Longest sequence length observed after clipping:     512\n",
            "05/29/2020 09:05:40 - INFO - farm.data_handler.data_silo -   Average sequence length after clipping: 391.05477528089887\n",
            "05/29/2020 09:05:40 - INFO - farm.data_handler.data_silo -   Proportion clipped:      0.33146067415730335\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEcTHpEvjRjx",
        "colab_type": "text"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiW91rS8jY-q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390,
          "referenced_widgets": [
            "fdbbe3b71aa24b89af699dbbf73f4883",
            "3e23057359954845b14f96cc70caa297",
            "bd0959de7eba423e80242f1276d36545",
            "86c6fbb89ee14cdfb832648ecfdfd897",
            "9e61b0e40b2045c2974eb18d925bcecd",
            "e0347805542f4008adce831054bf1616",
            "c8f0abbd6da34fb595464072f2f7435c",
            "6d83ee74dda645eda0da9af278d3ab3f",
            "7eb5ba9026f84ca899581c262d76edc6",
            "a7e35606265a471c901e71d678e7d3a3",
            "a10ed0c08166479e85d8a3bcf54b66db",
            "4641f0c406714a69aac7b0a28cab5045",
            "31f71400dfb54817ad08426fd2c64daa",
            "50c4d226138c40a8aeb29ff710ec29f2",
            "67f8df2427b441d2862216e90da93f5c",
            "129c98966aa2475ba8f0b346e6b271b7"
          ]
        },
        "outputId": "01c58648-46ba-4262-d721-851db4579fc3"
      },
      "source": [
        "# loading the pretrained BERT base cased model\n",
        "language_model = LanguageModel.load(lang_model)\n",
        "# prediction head for our model that is suited for classifying news article genres\n",
        "prediction_head = MultiLabelTextClassificationHead(num_labels=len(label_list))\n",
        "\n",
        "model = AdaptiveModel(\n",
        "        language_model=language_model,\n",
        "        prediction_heads=[prediction_head],\n",
        "        embeds_dropout_prob=0.1,\n",
        "        lm_output_types=[\"per_sequence\"],\n",
        "        device=device)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "05/29/2020 09:05:41 - INFO - filelock -   Lock 140456152004200 acquired on /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391.lock\n",
            "05/29/2020 09:05:41 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpzm6jjxcq\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fdbbe3b71aa24b89af699dbbf73f4883",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "05/29/2020 09:05:42 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json in cache at /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
            "05/29/2020 09:05:42 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
            "05/29/2020 09:05:42 - INFO - filelock -   Lock 140456152004200 released on /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391.lock\n",
            "05/29/2020 09:05:42 - INFO - filelock -   Lock 140456304074312 acquired on /root/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2.lock\n",
            "05/29/2020 09:05:42 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpdxakd19w\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7eb5ba9026f84ca899581c262d76edc6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "05/29/2020 09:05:48 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin in cache at /root/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
            "05/29/2020 09:05:48 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
            "05/29/2020 09:05:48 - INFO - filelock -   Lock 140456304074312 released on /root/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2.lock\n",
            "05/29/2020 09:05:48 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /root/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "05/29/2020 09:05:50 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
            "\t We guess it's an *ENGLISH* model ... \n",
            "\t If not: Init the language model by supplying the 'language' param.\n",
            "05/29/2020 09:05:50 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 5]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc2qiRLsjagC",
        "colab_type": "code",
        "outputId": "59e4c5aa-6e7a-441b-f579-4bb7ff752ea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "model, optimizer, lr_schedule = initialize_optimizer(\n",
        "        model=model,\n",
        "        learning_rate=3e-5,\n",
        "        device=device,\n",
        "        n_batches=len(data_silo.loaders[\"train\"]),\n",
        "        n_epochs=n_epochs)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "05/29/2020 09:05:59 - INFO - farm.modeling.optimization -   Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 3e-05}'\n",
            "05/29/2020 09:06:00 - INFO - farm.modeling.optimization -   Using scheduler 'get_linear_schedule_with_warmup'\n",
            "05/29/2020 09:06:00 - INFO - farm.modeling.optimization -   Loading schedule `get_linear_schedule_with_warmup`: '{'num_warmup_steps': 35.6, 'num_training_steps': 356}'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M83QIHF7jhd9",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMA6XumIjet_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        data_silo=data_silo,\n",
        "        epochs=n_epochs,\n",
        "        n_gpu=n_gpu,\n",
        "        lr_schedule=lr_schedule,\n",
        "        evaluate_every=evaluate_every,\n",
        "        device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIfCG5OhjkYK",
        "colab_type": "code",
        "outputId": "763e529c-8ad7-44e0-eb6c-366d8f6f5891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "05/29/2020 09:06:02 - INFO - farm.train -   \n",
            " \n",
            "\n",
            "          &&& &&  & &&             _____                   _             \n",
            "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
            "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
            "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
            "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
            "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
            " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
            "     &&     \\|||                                                   |___/\n",
            "             |||\n",
            "             |||\n",
            "             |||\n",
            "       , -=-~  .-^- _\n",
            "              `\n",
            "\n",
            "Train epoch 0/2 (Cur. train loss: 0.1582):  56%|█████▌    | 100/178 [00:49<00:35,  2.22it/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:06<00:00,  7.22it/s]\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "05/29/2020 09:06:58 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 100 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "05/29/2020 09:06:58 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "05/29/2020 09:06:59 - INFO - farm.eval -   loss: 0.11114195688219553\n",
            "05/29/2020 09:06:59 - INFO - farm.eval -   task_name: text_classification\n",
            "05/29/2020 09:06:59 - INFO - farm.eval -   f1_macro: 0.922606524518471\n",
            "05/29/2020 09:06:59 - INFO - farm.eval -   report: \n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "entertainment     0.9821    0.8462    0.9091        65\n",
            "        sport     0.9762    1.0000    0.9880        82\n",
            "     politics     0.7976    0.9853    0.8816        68\n",
            "     business     0.9848    0.8125    0.8904        80\n",
            "         tech     0.9219    0.9672    0.9440        61\n",
            "\n",
            "    micro avg     0.9266    0.9213    0.9239       356\n",
            "    macro avg     0.9325    0.9222    0.9226       356\n",
            " weighted avg     0.9358    0.9213    0.9238       356\n",
            "  samples avg     0.9185    0.9213    0.9195       356\n",
            "\n",
            "Train epoch 0/2 (Cur. train loss: 0.0221): 100%|██████████| 178/178 [01:34<00:00,  1.89it/s]\n",
            "Train epoch 1/2 (Cur. train loss: 0.0180):  12%|█▏        | 22/178 [00:10<01:07,  2.31it/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:06<00:00,  7.22it/s]\n",
            "05/29/2020 09:07:54 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 200 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "05/29/2020 09:07:54 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "05/29/2020 09:07:54 - INFO - farm.eval -   loss: 0.04578816334015868\n",
            "05/29/2020 09:07:54 - INFO - farm.eval -   task_name: text_classification\n",
            "05/29/2020 09:07:55 - INFO - farm.eval -   f1_macro: 0.9662715967529584\n",
            "05/29/2020 09:07:55 - INFO - farm.eval -   report: \n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "entertainment     0.9846    0.9846    0.9846        65\n",
            "        sport     0.9880    1.0000    0.9939        82\n",
            "     politics     0.9701    0.9559    0.9630        68\n",
            "     business     0.9740    0.9375    0.9554        80\n",
            "         tech     0.9344    0.9344    0.9344        61\n",
            "\n",
            "    micro avg     0.9717    0.9635    0.9676       356\n",
            "    macro avg     0.9702    0.9625    0.9663       356\n",
            " weighted avg     0.9716    0.9635    0.9675       356\n",
            "  samples avg     0.9635    0.9635    0.9635       356\n",
            "\n",
            "Train epoch 1/2 (Cur. train loss: 0.0169):  69%|██████▊   | 122/178 [01:05<00:24,  2.30it/s]\n",
            "Evaluating: 100%|██████████| 45/45 [00:06<00:00,  7.22it/s]\n",
            "05/29/2020 09:08:49 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | DEV SET | AFTER 300 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "05/29/2020 09:08:49 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "05/29/2020 09:08:50 - INFO - farm.eval -   loss: 0.04793298120997595\n",
            "05/29/2020 09:08:50 - INFO - farm.eval -   task_name: text_classification\n",
            "05/29/2020 09:08:50 - INFO - farm.eval -   f1_macro: 0.9696851544386543\n",
            "05/29/2020 09:08:50 - INFO - farm.eval -   report: \n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "entertainment     0.9848    1.0000    0.9924        65\n",
            "        sport     1.0000    1.0000    1.0000        82\n",
            "     politics     0.9429    0.9706    0.9565        68\n",
            "     business     0.9737    0.9250    0.9487        80\n",
            "         tech     0.9508    0.9508    0.9508        61\n",
            "\n",
            "    micro avg     0.9718    0.9691    0.9705       356\n",
            "    macro avg     0.9704    0.9693    0.9697       356\n",
            " weighted avg     0.9720    0.9691    0.9704       356\n",
            "  samples avg     0.9691    0.9691    0.9691       356\n",
            "\n",
            "Train epoch 1/2 (Cur. train loss: 0.0146): 100%|██████████| 178/178 [01:40<00:00,  1.77it/s]\n",
            "Evaluating: 100%|██████████| 56/56 [00:07<00:00,  7.17it/s]\n",
            "05/29/2020 09:09:25 - INFO - farm.eval -   \n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | TEST SET | AFTER 356 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "05/29/2020 09:09:25 - INFO - farm.eval -   \n",
            " _________ text_classification _________\n",
            "05/29/2020 09:09:25 - INFO - farm.eval -   loss: 0.051580544067232795\n",
            "05/29/2020 09:09:25 - INFO - farm.eval -   task_name: text_classification\n",
            "05/29/2020 09:09:26 - INFO - farm.eval -   f1_macro: 0.9698100655110103\n",
            "05/29/2020 09:09:26 - INFO - farm.eval -   report: \n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "entertainment     1.0000    0.9722    0.9859        72\n",
            "        sport     0.9902    0.9902    0.9902       102\n",
            "     politics     0.9136    0.9737    0.9427        76\n",
            "     business     0.9907    0.9217    0.9550       115\n",
            "         tech     0.9634    0.9875    0.9753        80\n",
            "\n",
            "    micro avg     0.9729    0.9663    0.9696       445\n",
            "    macro avg     0.9716    0.9691    0.9698       445\n",
            " weighted avg     0.9740    0.9663    0.9696       445\n",
            "  samples avg     0.9663    0.9663    0.9663       445\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaptiveModel(\n",
              "  (language_model): Bert(\n",
              "    (model): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (2): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (3): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (4): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (5): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (6): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (7): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (8): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (9): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (10): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (11): BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (pooler): BertPooler(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (activation): Tanh()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (prediction_heads): ModuleList(\n",
              "    (0): MultiLabelTextClassificationHead(\n",
              "      (feed_forward): FeedForwardBlock(\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=5, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (loss_fct): BCEWithLogitsLoss()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvGI__LP2Nm1",
        "colab_type": "text"
      },
      "source": [
        "## Saving and Inferencing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dgBoCYy2QhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_dir = \"saved_models/bert-english-news-article\"\n",
        "model.save(save_dir)\n",
        "processor.save(save_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV9mEdjLfYwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to download the model\n",
        "!zip -r saved_models/model.zip saved_models/bert-english-news-article"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1fZbw2XfqIS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "1bc105a8-d799-4d9d-ec4b-6387656542f5"
      },
      "source": [
        "inferenced_model = Inferencer.load(save_dir)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "05/29/2020 09:11:40 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
            "05/29/2020 09:11:40 - INFO - transformers.modeling_utils -   loading weights file saved_models/bert-english-news-article/language_model.bin from cache at saved_models/bert-english-news-article/language_model.bin\n",
            "05/29/2020 09:11:42 - INFO - farm.modeling.adaptive_model -   Found files for loading 1 prediction heads\n",
            "05/29/2020 09:11:42 - WARNING - farm.modeling.prediction_head -   `layer_dims` will be deprecated in future releases\n",
            "05/29/2020 09:11:42 - INFO - farm.modeling.prediction_head -   Prediction head initialized with size [768, 5]\n",
            "05/29/2020 09:11:42 - INFO - farm.modeling.prediction_head -   Loading prediction head from saved_models/bert-english-news-article/prediction_head_0.bin\n",
            "05/29/2020 09:11:42 - WARNING - farm.utils -   Failed to log params: INVALID_PARAMETER_VALUE: Changing param value is not allowed. Param with key='lm_name' was already logged with value='bert-base-cased' for run ID='2beb04b06c124bef84f766c5a661eaef. Attempted logging new value 'saved_models/bert-english-news-article'.\n",
            "05/29/2020 09:11:42 - INFO - transformers.tokenization_utils -   Model name 'saved_models/bert-english-news-article' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'saved_models/bert-english-news-article' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "05/29/2020 09:11:42 - INFO - transformers.tokenization_utils -   Didn't find file saved_models/bert-english-news-article/added_tokens.json. We won't load it.\n",
            "05/29/2020 09:11:42 - INFO - transformers.tokenization_utils -   loading file saved_models/bert-english-news-article/vocab.txt\n",
            "05/29/2020 09:11:42 - INFO - transformers.tokenization_utils -   loading file None\n",
            "05/29/2020 09:11:42 - INFO - transformers.tokenization_utils -   loading file saved_models/bert-english-news-article/special_tokens_map.json\n",
            "05/29/2020 09:11:42 - INFO - transformers.tokenization_utils -   loading file saved_models/bert-english-news-article/tokenizer_config.json\n",
            "05/29/2020 09:11:43 - INFO - farm.data_handler.processor -   Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for using the default task or add a custom task later via processor.add_task()\n",
            "05/29/2020 09:11:43 - INFO - farm.utils -   device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
            "05/29/2020 09:11:43 - INFO - farm.infer -   Got ya 1 parallel workers to do inference ...\n",
            "05/29/2020 09:11:43 - INFO - farm.infer -    0 \n",
            "05/29/2020 09:11:43 - INFO - farm.infer -   /w\\\n",
            "05/29/2020 09:11:43 - INFO - farm.infer -   /'\\\n",
            "05/29/2020 09:11:43 - INFO - farm.infer -   \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wuFlJc3iR_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_file(file_name: str) -> dict:\n",
        "  text_file = open (file_name, 'r')\n",
        "  text_file = text_file.read().replace('\\n', ' ')\n",
        "  return {'text': text_file}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFqAaZTAf4Sr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_input(text_files:list) -> list:\n",
        "  model_input = list()\n",
        "  for text_file in text_files:\n",
        "    model_input.append(read_file(text_file['file']))\n",
        "  return model_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c784qI2olhj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_result_overview (articles:list, result:list) -> pd.DataFrame:\n",
        "  files = list()\n",
        "  labels = list()\n",
        "  predictions = list()\n",
        "  for i in range(len(articles)):\n",
        "    files.append (articles[i]['file'])\n",
        "    labels.append(articles[i]['genre'])\n",
        "    predictions.append(result[0]['predictions'][i]['label'].strip(\"'[]'\"))\n",
        "  data = {'file': files, 'actual': labels, 'prediction': predictions}\n",
        "  df = pd.DataFrame(data)\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8WC2-WBk1BW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "1db600c2-83d2-4435-83cb-d05ea52b78fb"
      },
      "source": [
        "articles = [{'file': 'bbc_news/generated_data/inferencing/business.txt', 'genre': 'business'},\n",
        "            {'file': 'bbc_news/generated_data/inferencing/sport.txt', 'genre': 'sport'}]\n",
        "\n",
        "article_texts = create_input(articles)\n",
        "\n",
        "result = inferenced_model.inference_from_dicts(article_texts)\n",
        "\n",
        "df = create_result_overview(articles, result)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.54s/ Batches]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file</th>\n",
              "      <th>actual</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bbc_news/generated_data/inferencing/business.txt</td>\n",
              "      <td>business</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bbc_news/generated_data/inferencing/sport.txt</td>\n",
              "      <td>sport</td>\n",
              "      <td>sport</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               file    actual prediction\n",
              "0  bbc_news/generated_data/inferencing/business.txt  business   business\n",
              "1     bbc_news/generated_data/inferencing/sport.txt     sport      sport"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWPFSanmvJjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}